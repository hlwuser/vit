{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T05:43:48.709408Z","iopub.execute_input":"2026-01-16T05:43:48.709659Z","iopub.status.idle":"2026-01-16T05:43:50.516787Z","shell.execute_reply.started":"2026-01-16T05:43:48.709630Z","shell.execute_reply":"2026-01-16T05:43:50.515949Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:30:49.529703Z","iopub.execute_input":"2026-01-15T16:30:49.530271Z","iopub.status.idle":"2026-01-15T16:30:52.771377Z","shell.execute_reply.started":"2026-01-15T16:30:49.530240Z","shell.execute_reply":"2026-01-15T16:30:52.770785Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport json\nimport os\n\n# ==================== MODEL COMPONENTS ====================\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\n\nclass PatchEmbeddings(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.patch_size = cfg[\"patch\"]\n        self.img_size = cfg[\"image_size\"]\n        self.hidden_size = cfg[\"dim\"]\n        self.no_patch = (self.img_size // self.patch_size) ** 2\n        self.layer = nn.Conv2d(3, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n    \n    def forward(self, x):\n        x = self.layer(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass Embeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.patch_embed = PatchEmbeddings(config)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"dim\"]))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.no_patch + 1, config[\"dim\"]))\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        batch_size, _, _ = x.shape\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = self.pos_embed + x\n        return x\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, inp_dim, cfg):\n        super().__init__()\n        self.patch_size = cfg[\"patch\"]\n        self.img_size = cfg[\"image_size\"]\n        self.context_length = (self.img_size // self.patch_size) ** 2 + 1\n        self.inp_dim = inp_dim\n        self.w_q = nn.Linear(inp_dim, inp_dim)\n        self.w_k = nn.Linear(inp_dim, inp_dim)\n        self.w_v = nn.Linear(inp_dim, inp_dim)\n        self.dropout = nn.Dropout(0.1)\n        self.attn_weights = None  # Store attention weights\n    \n    def forward(self, x):\n        b, p, d = x.shape\n        q = self.w_q(x)\n        k = self.w_k(x)\n        v = self.w_v(x)\n        attn = q @ k.transpose(1, 2)\n        dim_k = k.shape[-1]\n        attn = attn / dim_k ** 0.5\n        attn_scores = torch.softmax(attn, dim=-1)\n        self.attn_weights = attn_scores  # Save for visualization\n        attn_scores = self.dropout(attn_scores)\n        return attn_scores @ v\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_heads, inp_dim, cfg):\n        super().__init__()\n        self.head_dim = inp_dim // n_heads\n        self.n_heads = n_heads\n        self.heads = nn.ModuleList([SelfAttention(self.head_dim, cfg) for _ in range(n_heads)])\n    \n    def forward(self, x):\n        # Split input for each head\n        batch_size, seq_len, dim = x.shape\n        x_split = x.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        x_split = x_split.transpose(1, 2)  # (batch, n_heads, seq_len, head_dim)\n        \n        outputs = []\n        for i, head in enumerate(self.heads):\n            head_input = x_split[:, i, :, :].contiguous()\n            outputs.append(head(head_input))\n        \n        x = torch.cat(outputs, dim=-1)\n        return x\n    \n    def get_attention_weights(self):\n        \"\"\"Get attention weights from all heads\"\"\"\n        return [head.attn_weights for head in self.heads if head.attn_weights is not None]\n\n\nclass MLP(nn.Module):\n    def __init__(self, inp_dim, drop_rate):\n        super().__init__()\n        self.layer1 = nn.Linear(inp_dim, 4 * inp_dim)\n        self.layer2 = nn.Linear(4 * inp_dim, inp_dim)\n        self.dropout = nn.Dropout(drop_rate)\n        self.activation = GELU()\n    \n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.activation(x)\n        x = self.layer2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, n_heads, cfg):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.attn = MultiHeadAttention(n_heads, dim, cfg)\n        self.mlp = MLP(dim, 0.1)\n    \n    def forward(self, x):\n        residue = x\n        x = self.norm1(x)\n        x = self.attn(x)\n        x = residue + x\n        residue = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = residue + x\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            Transformer(cfg[\"dim\"], cfg[\"n_heads\"], cfg) for _ in range(cfg[\"layers\"])\n        ])\n        self.embedding = Embeddings(cfg)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x\n    \n    def get_attention_weights(self):\n        \"\"\"Get attention weights from all layers\"\"\"\n        all_attentions = []\n        for layer in self.layers:\n            all_attentions.append(layer.attn.get_attention_weights())\n        return all_attentions\n\n\nclass ViT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.image_size = config[\"image_size\"]\n        self.hidden_size = config[\"dim\"]\n        self.num_classes = config[\"num_classes\"]\n        self.encoder = Encoder(config)\n        self.out = nn.Linear(config[\"dim\"], self.num_classes)\n    \n    def forward(self, x):\n        x = self.encoder(x)\n        cls = x[:, 0, :]\n        logits = self.out(cls)\n        return logits\n    \n    def get_attention_weights(self):\n        \"\"\"Get all attention weights for visualization\"\"\"\n        return self.encoder.get_attention_weights()\n\n\n# ==================== DATA PREPARATION ====================\n\ndef prepare_data(batch_size=32, num_workers=2):\n    train_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((64, 64)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomResizedCrop((64, 64), scale=(0.8, 1.0)),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                            download=True, transform=train_transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                              shuffle=True, num_workers=num_workers)\n\n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((64, 64)),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                           download=True, transform=test_transform)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                             shuffle=False, num_workers=num_workers)\n\n    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n    return trainloader, testloader, classes\n\n\n# ==================== UTILITIES ====================\n\ndef save_experiment(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir=\"experiments\"):\n    outdir = os.path.join(base_dir, experiment_name)\n    os.makedirs(outdir, exist_ok=True)\n\n    with open(os.path.join(outdir, 'config.json'), 'w') as f:\n        json.dump(config, f, sort_keys=True, indent=4)\n\n    with open(os.path.join(outdir, 'metrics.json'), 'w') as f:\n        data = {\n            'train_losses': train_losses,\n            'test_losses': test_losses,\n            'accuracies': accuracies,\n        }\n        json.dump(data, f, sort_keys=True, indent=4)\n\n    torch.save(model.state_dict(), os.path.join(outdir, 'model_final.pt'))\n\n\n# ==================== TRAINER ====================\n\nclass Trainer:\n    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n        self.model = model.to(device)\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n        self.exp_name = exp_name\n        self.device = device\n\n    def train(self, trainloader, testloader, epochs):\n        train_losses, test_losses, accuracies = [], [], []\n        \n        for epoch in range(epochs):\n            train_loss = self.train_epoch(trainloader)\n            accuracy, test_loss = self.evaluate(testloader)\n            train_losses.append(train_loss)\n            test_losses.append(test_loss)\n            accuracies.append(accuracy)\n            print(f\"Epoch: {epoch+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n        \n        save_experiment(self.exp_name, self.model.config, self.model, train_losses, test_losses, accuracies)\n        return train_losses, test_losses, accuracies\n\n    def train_epoch(self, trainloader):\n        self.model.train()\n        total_loss = 0\n        for images, labels in trainloader:\n            images, labels = images.to(self.device), labels.to(self.device)\n            self.optimizer.zero_grad()\n            logits = self.model(images)\n            loss = self.loss_fn(logits, labels)\n            loss.backward()\n            self.optimizer.step()\n            total_loss += loss.item() * len(images)\n        return total_loss / len(trainloader.dataset)\n\n    @torch.no_grad()\n    def evaluate(self, testloader):\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        for images, labels in testloader:\n            images, labels = images.to(self.device), labels.to(self.device)\n            logits = self.model(images)\n            loss = self.loss_fn(logits, labels)\n            total_loss += loss.item() * len(images)\n            predictions = torch.argmax(logits, dim=1)\n            correct += torch.sum(predictions == labels).item()\n        \n        accuracy = correct / len(testloader.dataset)\n        avg_loss = total_loss / len(testloader.dataset)\n        return accuracy, avg_loss\n\n\n# ==================== ATTENTION VISUALIZATION ====================\n\ndef visualize_attention(model, image, label, classes, device, layer_idx=0, head_idx=0):\n    \"\"\"\n    Visualize attention weights for a single image\n    \n    Args:\n        model: Trained ViT model\n        image: Input image tensor (C, H, W)\n        label: True label\n        classes: List of class names\n        device: Device to run on\n        layer_idx: Which transformer layer to visualize\n        head_idx: Which attention head to visualize\n    \"\"\"\n    model.eval()\n    \n    # Add batch dimension\n    image_batch = image.unsqueeze(0).to(device)\n    \n    # Forward pass\n    with torch.no_grad():\n        logits = model(image_batch)\n        prediction = torch.argmax(logits, dim=1).item()\n        \n        # Get attention weights\n        all_attentions = model.get_attention_weights()\n    \n    # Get specific layer and head attention\n    if layer_idx < len(all_attentions) and head_idx < len(all_attentions[layer_idx]):\n        attn_weights = all_attentions[layer_idx][head_idx][0].cpu().numpy()  # [seq_len, seq_len]\n    else:\n        print(f\"Invalid layer_idx or head_idx\")\n        return\n    \n    # Prepare image for display\n    img_display = image.permute(1, 2, 0).cpu().numpy()\n    img_display = (img_display * 0.5) + 0.5  # Denormalize\n    img_display = np.clip(img_display, 0, 1)\n    \n    # Create visualization\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Original image\n    axes[0].imshow(img_display)\n    axes[0].set_title(f'Original Image\\nTrue: {classes[label]}\\nPred: {classes[prediction]}')\n    axes[0].axis('off')\n    \n    # CLS token attention (how CLS attends to patches)\n    cls_attn = attn_weights[0, 1:]  # Exclude CLS to CLS\n    num_patches = int(np.sqrt(len(cls_attn)))\n    cls_attn_map = cls_attn.reshape(num_patches, num_patches)\n    \n    im1 = axes[1].imshow(cls_attn_map, cmap='viridis')\n    axes[1].set_title(f'CLS Token Attention\\nLayer {layer_idx+1}, Head {head_idx+1}')\n    axes[1].axis('off')\n    plt.colorbar(im1, ax=axes[1], fraction=0.046)\n    \n    # Mean attention across all patches\n    mean_attn = attn_weights[1:, 1:].mean(axis=0)  # Average over query positions\n    mean_attn_map = mean_attn.reshape(num_patches, num_patches)\n    \n    im2 = axes[2].imshow(mean_attn_map, cmap='viridis')\n    axes[2].set_title(f'Mean Patch Attention\\nLayer {layer_idx+1}, Head {head_idx+1}')\n    axes[2].axis('off')\n    plt.colorbar(im2, ax=axes[2], fraction=0.046)\n    \n    plt.tight_layout()\n    plt.savefig(f'attention_visualization_L{layer_idx}_H{head_idx}.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\ndef visualize_multiple_heads(model, image, label, classes, device, layer_idx=0, num_heads=4):\n    \"\"\"\n    Visualize attention from multiple heads in a grid\n    \"\"\"\n    model.eval()\n    image_batch = image.unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        logits = model(image_batch)\n        prediction = torch.argmax(logits, dim=1).item()\n        all_attentions = model.get_attention_weights()\n    \n    # Prepare image\n    img_display = image.permute(1, 2, 0).cpu().numpy()\n    img_display = (img_display * 0.5) + 0.5\n    img_display = np.clip(img_display, 0, 1)\n    \n    # Create grid\n    fig, axes = plt.subplots(2, num_heads // 2 + 1, figsize=(20, 8))\n    axes = axes.flatten()\n    \n    # Show original image\n    axes[0].imshow(img_display)\n    axes[0].set_title(f'Original\\nTrue: {classes[label]}\\nPred: {classes[prediction]}', fontsize=10)\n    axes[0].axis('off')\n    \n    # Show attention from each head\n    for head_idx in range(min(num_heads, len(all_attentions[layer_idx]))):\n        attn_weights = all_attentions[layer_idx][head_idx][0].cpu().numpy()\n        cls_attn = attn_weights[0, 1:]\n        num_patches = int(np.sqrt(len(cls_attn)))\n        cls_attn_map = cls_attn.reshape(num_patches, num_patches)\n        \n        im = axes[head_idx + 1].imshow(cls_attn_map, cmap='viridis')\n        axes[head_idx + 1].set_title(f'Head {head_idx+1}', fontsize=10)\n        axes[head_idx + 1].axis('off')\n        plt.colorbar(im, ax=axes[head_idx + 1], fraction=0.046, pad=0.04)\n    \n    # Hide unused subplots\n    for idx in range(num_heads + 1, len(axes)):\n        axes[idx].axis('off')\n    \n    plt.suptitle(f'Attention Heads - Layer {layer_idx+1}', fontsize=14, y=1.02)\n    plt.tight_layout()\n    plt.savefig(f'attention_all_heads_L{layer_idx}.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\n# ==================== MAIN ====================\n\ndef main():\n    # Configuration\n    config = {\n        \"patch\": 4,\n        \"dim\": 128,\n        \"n_heads\": 16,\n        \"layers\": 16,\n        \"image_size\": 64,\n        \"num_classes\": 10,\n    }\n    \n    # Training parameters\n    exp_name = 'vit-cifar10'\n    batch_size = 32\n    epochs = 10\n    lr = 1e-2\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    print(f\"Using device: {device}\")\n    print(f\"Configuration: {config}\")\n    \n    # Prepare data\n    trainloader, testloader, classes = prepare_data(batch_size=batch_size)\n    \n    # Create model\n    model = ViT(config)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n    loss_fn = nn.CrossEntropyLoss()\n    \n    # Train\n    trainer = Trainer(model, optimizer, loss_fn, exp_name, device=device)\n    print(\"\\nStarting training...\")\n    train_losses, test_losses, accuracies = trainer.train(trainloader, testloader, epochs)\n    \n    # Visualize attention on test samples\n    print(\"\\nGenerating attention visualizations...\")\n    model.eval()\n    \n    # Get a few test images\n    test_iter = iter(testloader)\n    images, labels = next(test_iter)\n    \n    # Visualize first 3 images\n    for i in range(min(3, len(images))):\n        print(f\"\\nVisualizing sample {i+1}\")\n        visualize_attention(model, images[i], labels[i].item(), classes, device, layer_idx=0, head_idx=0)\n        visualize_multiple_heads(model, images[i], labels[i].item(), classes, device, layer_idx=0, num_heads=4)\n    \n    print(\"\\nTraining complete! Attention visualizations saved.\")\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T07:13:25.177244Z","iopub.execute_input":"2026-01-16T07:13:25.178062Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nConfiguration: {'patch': 4, 'dim': 128, 'n_heads': 16, 'layers': 16, 'image_size': 64, 'num_classes': 10}\n\nStarting training...\nEpoch: 1, Train loss: 2.2953, Test loss: 1.6340, Accuracy: 0.3778\nEpoch: 2, Train loss: 1.5619, Test loss: 1.4235, Accuracy: 0.4718\nEpoch: 3, Train loss: 21.5374, Test loss: 12.7163, Accuracy: 0.1532\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}